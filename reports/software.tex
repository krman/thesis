\chapter{Software Design}

SDN is mature enough technology that there are a number of different software packages and frameworks that are available to support development. Specifically we are interested in the choice of controller framework, software for the switch, and a way of running reproducible network experiments on large-enough-to-be-interesting topologies.

\section{Dependencies}
\subsection{POX}
There are a number of controller frameworks today which allow programmers to send OpenFlow messages to switches in the network; selection in the first case is based on familiarity with the development language used. One such framework, based on Python, is POX \cite{onl:pox}, developed at Stanford primarily for ease of research over speed and performance. This is acceptable if performance of the network is not bound by the controller, at least for initial development; this can be verified if necessary.

only supports openflow 1.0

\subsection{OpenVSwitch}

\subsection{Mininet}
Mininet is a network emulator which uses container-based emulation \cite{handigol:mininet} to create a virtual network of hosts and switches. Mininet makes it simple to run reproducible network experiments under realistic conditions, with topologies and behaviour programmable in Python. The resources allocated to each host can be controlled and monitored, and benchmarking \cite{handigol:benchmarks} has generally shown that the performance and timing characteristics are accurate. OpenFlow-enabled switches such as OpenVSwitch can be used in the simulations so the system is ideal for SDN research.

\subsection{GLPK/PuLP}
glpk is a solver written in C for solving mixed integer programs among other things. extensible so i can implement new algorithms.

pulp is a way of writing python programs which call glpk solvers. have been using python everywhere else so might as well be consistent, and python is great.

\subsection{NetworkX}
NetworkX is a graph library which is stable and well-maintained and has a flexible, easy-to-read annotation system for nodes and edges, which is used to store information such as IP addresses and link capacities.

\section{POX Modules}
The major software contribution of this thesis lies in the series of POX modules that make up the mcfpox controller. The central base module launches the other three modules: one each for topology discovery, statistics gathering and routing control (described in detail below).

In order to launch the controller programmatically in experiment scripts, an important modification was made to the startup sequence of POX. Up to the most recent branch on the noxrepo repository, dart, POX is only designed to be started from the command line. Though the command line interface provides a format for passing arguments to individual modules, this means that arguments must be serialised to text, and are limited in length to the maximum length of arguments. However, arguments to mcfpox modules can be quite complex; for example, the multicommodity module accepts an objective function and a dictionary of flow rules to install. In initial development of mcfpox this was achieved by specifying the name of a Python module to import containing this data, but this imposes restrictions on the layout of the module; for example, the objective function must have a predefined name to make the import process easier. The POX initialisation code was therefore modified to allow starting the controller directly from a Python script, which allows the passing of arbitrary Python objects such as functions and dictionaries directly as arguments, without the need for serialisation to the command line. Correspondence with the principal maintainer of POX, Murphy McCauley, indicated that he is interested in possibly incorporating this feature into a later release; until then the mcfpox project officially relies on the github.com/krman fork of POX. 

\subsection{Network Discovery}
Network discovery of switches, hosts, and links is done using the existing POX modules discovery and host\_tracker. The topology module uses these two modules to build a NetworkX graph which is passed to the objective function for routing calculations.

The discovery module uses LLDP packets to discover switches and links between them, which are specified with the same dpid that switches use to identify themselves to the controller. The module builds up an adjacency list of pairs of switches which are connected, including the ports by which the switches are connected. The host\_tracker module tracks ARP requests and replies and uses them to locate hosts in the network, but stores this information in a significantly different format to discovery; this makes it difficult to use both modules together. The main purpose of the topology module is therefore to bridge these two modules, by maintaining a NetworkX graph containing information about both hosts and switches. NetworkX allows annotations on both nodes and edges in the graph; these are used to store information on IP addresses, switch ports and link capacities.

The information provided by discovery and host\_tracker is limited: both only specify that a link exists, but not important properties of the link such as its capacity. In the current implementation, all links between switches are recorded as having a 10 Mbps capacity, regardless of actual capacity; to make this work, all Mininet topologies are created with 10 Mbps links. This limits the types of topologies that can be tested. While this is obviously undesirable, capacity discovery is a significant problem of its own. Another student in Marius Portmann's SDN group is currently researching using packet-pair probing and other techniques to dynamically discover link capacities; as such this is outside the scope of this thesis. An alternative approach to this, to allow for calculations of routes to incorporate different capacities without having to discover the network, would be to allow a mock network graph to be passed to the controller at startup time. This could only be considered a legitimate solution in an experimental setting, of course.

\subsection{Flow Statistics}
OpenFlow provides a number of message types to query switch statistics, such as number of packets seen for particular flows. The statistics module sends FlowStatsRequest messages to each switch known by topology periodically (the time between requests is configurable at startup). Each switch replies with a FlowStatsReply, which lists the number of bytes which have been processed for each flow corresponding to an installed flow rule. Using this information, the statistics module calculates the number of bits per second seen, recently, per flow. Since the replies are per switch, occasionally the figures differ very slightly (for example, if a flow has only partially traversed the network); in such cases the statistics module records the greatest number seen on any switch.

In order to facilitate the measurement of statistics, flows are initially routed using the simple shortest-path metric. This allows the statistics module to record an estimate of the flow size, before full-network routing is calculated. While this works well when the network is not under load, as the number of flows in the network increases the statistics become less accurate over time. This is due to the difference between what HEDERA calls the natural demand and the measured demand: if flows are initially routed badly, then congestion will occur. The TCP sending rate* algorithm will slow down the sending rate to avoid excessive packet loss, which means that although switches are accurately reporting the bytes passing through, this number may not accurately reflect how much the sender would transmit given unlimited network capacity. Continuing, the globally-optimal routing metrics will then underestimate the size of that flow, and perhaps route it inefficiently, instead of allocating it to a bigger link where it could transmit at a greater speed. This reduces the overall throughput in the network. The authors of HEDERA implemented an algorithm to estimate the natural demand of a flow, but similarly to link capacity detection, this is outside the scope of this project and could be implemented by a later student.

\subsection{Routing Control}
The final but most important mcfpox module is multicommodity, the routing control module. Routing is controlled in two stages. When a new flow arrives, it is assigned an interim path and rules are installed on switches along this path. Periodically (how often is configurable) the module runs an objective function, which calculates optimal routes for all current (recently-seen) flows, removes existing switch rules for these flows and installs the new ones. The interim path is simply the shortest path between source and destination nodes.

The objective function is configured at startup time. After a period of time, the multicommodity module takes a snapshot of the current view of the network from the topology module, and the most recent flow statistics from the statistics module, and passes these to the given objective function, which uses them to calculate the set of routes for each flow, expressed as a list of hops (see section SECTION for more information on objective functions). For each calculated path, expressed as a list of hops, the multicommodity module installs a rule on each switch in the path that forwards packets corresponding to that flow out on the appropriate port.

As described in section SECTION, the iperf clients which form the flows in the network begin measuring throughput 10 seconds after being launched, concurrently with the launch of POX. The recalculation of forwarding rules is timed to coincide with this point, so that the iperf measurements are based on the new paths rather than the initial shortest-path routes.  In the current implementation, the controller is stopped after only after one recalculation, as this is all that is required for the experiments, but in a normal situation this would occur repeatedly, as new flows entered and left the network.

One feature of the multicommodity module that was not used for the final experiments but was extensively used in development was the ability to install mock flow rules, in order to test the throughput in simple networks with known paths, to check that the experiment framework worked as expected. Flow rules could be calculated externally or manually written, then passed to the controller and directly installed. This can also be used as an alternative to shortest path for initial routing.
