\addtocontents{toc}{\protect\pagebreak}
\chapter{Controller Design}

\section{Architecture}
A major deliverable for this project is the production of a configurable SDN controller, to allow testing of different routing metrics or objective functions. A high-level overview of the system architecture is given in Figure \ref{fig:arch}. The controller framework consists of three modules, described individually in detail later: \texttt{topology}, \texttt{statistics} and \texttt{multicommodity}. While the objective function is configurable, it can be considered part of the controller as well.

\subsection{Communication}
Immediately before running the objective function, the multicommodity module proactively requests the network graph and list of current flows. The topology module uses adjacency lists from the \texttt{discovery} and \texttt{host\_tracker} modules to return an updated graph. Finally, the multicommodity module calls the provided objective function with the graph and flows as arguments, and receives a list of paths for each flow back. This list is used to install forwarding rules on the switches.

\begin{landscape}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.85,trim=0 30mm 0 10mm]{../images/architecture.pdf}
  \caption{Overview of controller architecture}
  \label{fig:arch}
\end{figure}
\end{landscape}

The controller communicates with switches in the network via the OpenFlow protocol. The statistics module exchanges \texttt{OFPT\_STATS\_REQUEST/REPLY} modules periodically to update the list of flows in the network, including their size over the most recent period. After paths are calculated, the multicommodity module sends \texttt{OFPT\_FLOW\_MOD} messages to instruct the appropriate switches to update their forwarding tables. 

Additionally, while the topology module does not send or receive OpenFlow messages directly, it makes use of information sourced from the \texttt{discovery} and \texttt{host\_tracker} modules in POX; the former uses \texttt{OFPT\_PACKET\_OUT} messages to send and track LLDP messages for switch discovery, and the latter inspects \texttt{OFPT\_PACKET\_IN} messages for ARP messages indicating the location of hosts. These messages are not shown in Figure \ref{fig:arch} as they are not controlled by \thesis{} modules.

\section{Dependencies}
\subsection{POX}
There are a number of controller frameworks today which allow programmers to send OpenFlow messages to switches in the network; selection in the first case is based on familiarity with the development language used. One such framework, based on Python, is POX \cite{onl:pox}, developed at Stanford primarily for ease of research over speed and performance. POX only supports OpenFlow 1.0 \cite{onf:switch100}.

Functionality in POX is implemented as reuseable modules, which can be combined with other modules to meet particular requirements. POX ships with a number of prewritten modules for basic switch forwarding, network discovery and so on; of particular interest are the \texttt{openflow.discovery} and \texttt{host\_tracker.host\_tracker} modules, which are used by the \thesis{} topology module for network discovery.


\subsection{NetworkX}
NetworkX is a Python package for the `creation, manipulation, and study of the structure, dynamics, and functions of complex networks'. It is a stable and well-maintained graph library with a flexible, easy-to-read annotation system for nodes and edges, which is used to store information such as IP addresses and link capacities. \thesis{} uses NetworkX to store a representation of the network as discovered by the controller. The topology module builds a NetworkX graph, which is passed to the objective function; it can then make use of provided mathematical functions, such as the \texttt{shortest\_path} and \texttt{all\_simple\_paths} functions.

\subsection{GLPK/PuLP}
Finally, GLPK is a general solver for optimisation problems, including mixed integer problems such as the residual capacity metric described in section \ref{sec:rc}. GLPK is written in C, for speed and efficiency reasons, and uses techniques 

PuLP is a Python module which is used to describe optimisation problems programmatically. After an objective function and constraints are added, PuLP writes the description of the problem to a file, and then calls an external solver, such as GLPK, to solve the model and return the optimal solution. The interface is entirely through Python code, which fits well with the rest of the framework.

\section{POX Modules}
The major software contribution of this thesis lies in the series of POX modules that make up the \thesis{} controller. The central base module launches the other three modules: one each for topology discovery, statistics gathering and routing control (described in detail below). Of these, the most relevant is the routing control module, which allows the routing metric under study to be passed to the controller. The other two modules are essential prerequisites for correct operation of the routing module, but not the major focus of this thesis.

In order to launch the controller programmatically in experiment scripts, an important modification was made to the startup sequence of POX. As of the most recent branch on the \url{github.com/noxrepo} repository, \texttt{dart}, POX is only designed to be started from the command line. Though the command line interface provides a format for passing arguments to individual modules, arguments must be serialised to text, and are limited in length to the maximum length of arguments. However, arguments to mcfpox modules can be quite complex; for example, the multicommodity module accepts an objective function and a dictionary of flow rules to install.

In initial development of mcfpox this was achieved by specifying the name of a Python module to import containing this data, but this imposes restrictions on the layout of the module; for example, the objective function must have a predefined name to make the import process easier. The POX initialisation code was therefore modified to allow starting the controller directly from a Python script, which allows the passing of arbitrary Python objects such as functions and dictionaries directly as arguments, without the need for serialisation to the command line. Correspondence with the principal maintainer of POX, Murphy McCauley, indicated that he is interested in possibly incorporating this feature into a later release; until then the mcfpox project officially relies on the \url{github.com/krman} fork of POX. 

\subsection{Network Discovery}
Network discovery of switches, hosts, and links is done using the existing POX modules \texttt{discovery} and \texttt{host\_tracker}. The topology module uses these two modules to build a NetworkX graph of the underlying network, which is passed to the objective function for routing calculations.

The \texttt{discovery} module uses LLDP packets to discover switches and links between them, which are specified with the same dpid that switches use to identify themselves to the controller. The module builds up an adjacency list of pairs of switches which are connected, including the ports by which the switches are connected. The \texttt{host\_tracker} module tracks ARP requests and replies and uses them to locate hosts in the network, but stores this information in a significantly different format to \texttt{discovery}; this makes it difficult to use both modules together. The main purpose of the topology module is therefore to bridge these two modules, by maintaining a NetworkX graph containing information about both hosts and switches. NetworkX allows annotations on both nodes and edges in the graph; these are used to store information on IP addresses, switch ports and link capacities.

The information provided by \texttt{discovery} and \texttt{host\_tracker} is limited: both only specify that a link exists, but not important properties of the link such as its capacity. In the current implementation, all links between switches are recorded as having a 10 Mbps capacity, regardless of actual capacity; to make this work, all Mininet topologies are created with 10 Mbps links. This limits the types of topologies that can be tested. While this is obviously undesirable, capacity discovery is a significant problem of its own. Another student in Marius Portmann's SDN group is currently researching using packet-pair probing and other techniques to dynamically discover link capacities; as such this is outside the scope of this thesis. 

\subsection{Flow Statistics}
OpenFlow provides a number of message types to query switch statistics, such as number of packets or number of bytes seen for particular flows. The statistics module sends \texttt{OFPT\_STATS\_REQUEST} messages to each switch known by the controller periodically (the time between requests is configurable at startup). Each switch replies with a \texttt{OFPT\_STATS\_REPLY}, which lists the number of bytes which have been processed for each flow corresponding to an installed flow rule. Using this information, the statistics module calculates the number of bits per second seen, recently, per flow. Since the replies are per switch, occasionally the figures differ very slightly (for example, if a flow has only partially traversed the network); in such cases the statistics module records the greatest number seen on any switch.

In order to facilitate the measurement of statistics, flows are initially routed using the simple shortest-path metric. This allows the statistics module to record an estimate of the flow size, before full-network routing is calculated. While this works well when the network is not under load, as the number of flows in the network increases the statistics become less accurate over time. This is due to the difference between what the authors of a similar framework \cite{alfares:hedera} refer to as the \emph{natural demand} and the \emph{measured demand}: if flows are initially routed badly, then congestion will occur. The TCP congestion control algorithm will slow down the sending rate to avoid excessive packet loss, which means that although switches are accurately reporting the bytes passing through, this number may not accurately reflect how much the sender would transmit given unlimited network capacity. 

Continuing, the globally-optimal routing metrics will then underestimate the size of that flow, and perhaps route it inefficiently, instead of allocating it to a bigger link where it could transmit at a greater speed. This reduces the overall throughput in the network. The authors of the Hedera framework \cite{alfares:hedera} implemented an algorithm to estimate the natural demand of a flow, but similarly to link capacity detection, this is outside the scope of this project.

\subsection{Routing Control}
The final but most important mcfpox module is multicommodity, the routing control module. Routing is controlled in two stages. When a new flow arrives, it is assigned an interim path and rules are installed on switches along this path. Periodically (how often is configurable, though for experiments described in this thesis it is only run once) the module runs an objective function, which calculates optimal routes for all current (recently-seen) flows, removes existing switch rules for these flows and installs the new ones. The interim path is simply the shortest path between source and destination nodes.

The objective function is configured at startup time. After a period of time, the multicommodity module takes a snapshot of the current view of the network from the topology module, and the most recent flow statistics from the statistics module, and passes these to the given objective function, which uses them to calculate the set of routes for each flow. For each calculated path, expressed as a list of hops, the multicommodity module installs a rule on each switch in the path that forwards packets corresponding to that flow out on the appropriate port.

As described later in Chapter \ref{ch:methods}, the iperf clients which form the flows in the network begin measuring throughput 10 seconds after being launched, concurrently with the launch of POX. The recalculation of forwarding rules is timed to coincide with this point, so that the iperf measurements are based on the new paths rather than the initial shortest-path routes.  In the current implementation, the controller is stopped after only after one recalculation, as this is all that is required for the experiments, but in a normal situation this would occur repeatedly, as new flows entered and left the network.

One feature of the multicommodity module that was not used for the final experiments but was extensively used in development was the ability to install mock flow rules, in order to test the throughput in simple networks with known paths, to check that the experiment framework worked as expected. Flow rules could be calculated externally or manually written, then passed to the controller and directly installed. This can also be used as an alternative to shortest path for initial routing.
