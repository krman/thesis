\chapter{Experimental Methods}

The goal of this thesis is to implement a configurable controller which can be used in network experiments to compare the performance of different routing algorithms. For the purposes of this thesis, performance is measured exclusively in terms of overall throughput in the network. In order to measure throughput for different routing metrics, a series of network experiments are devised. Chapter CHAPTER outlines the basic layout of the controller. Chapter CHAPTER outlines the three routing metrics which will be compared; the shortest-path metric, given its simplicity and the number of traditional routing metrics which are based on it, can be considered a control or benchmark by which the other two metrics can be compared.

All network experiments follow a defined pattern. Figure FIGURE shows the timing of each experiment trial. Firstly, POX and Mininet are started. There is a non-trivial setup time for Mininet topologies to be created, followed by a non-trivial time for POX's discovery module to discover the network. After waiting for this to occur (20 seconds is enough time for the largest topology considered for this thesis), a series of iperf flows are started, limited to a specified bandwidth. For the first 10 seconds, iperf simply sends data at a maximum of this specified bandwidth, but ignores the number of bytes sent for this period. This is because the throughput for the flow will be lower than expected, for several reasons: the TCP slow-start period, the time taken for ARP requests and replies to be exchanged, and for the controller to be notified of the hosts involved and set an initial path. After 10 seconds, the controller's multicommodity module recalculates all routes in the network according to the objective function used, and replaces the initial flow rules with updated forwarding rules based on these routes. The iperf clients continue to send data at the same rates for this period, but begin to record the number of bytes sent and received, and do so for 10 seconds. A few seconds after this, to give iperf time to write this data to log files, the experiment concludes and the controller and Mininet are shut down, including all iperf servers and clients still running.

The topology, flow pattern and routing metric considered can all be configured via a simple launch script, such as the sample shown in Figure FIGURE. The following sections explain how to firstly select these parameters so that useful information can be obtained, and secondly to write experiment scripts to run the desired experiment. 

\section{Experiment Design}
While it is possible to use mcfpox to run experiments using many combinations of topologies and flow patterns, not all of these combinations will yield useful data. For example, if all flows in a pattern are much larger or smaller than the link capacities in the topology, either all flows will be constrained or none will be. A carefully-selected combination, however, will yield a situation where flows will only be constrained if an inefficient routing metric is used, but not otherwise. In such a situation it is possible to measure the variation in performance for the two routing metrics.

As noted in ANOTHER SECTION, due to constraints on link discovery all implemented topologies have 10 Mbps links between switches. The experiments in this thesis therefore examine flows with sizes ranging from 1 - 9 Mbps. The exact flow patterns used depend on the topology:

- pentagon: two flows between h1 and h2 of the same size, in the range [1,9] Mbps.
- manhattan, alfares: each host connected to one random other, flows of the same size in the range [1-9] Mbps

The pentagon topology is used as a test of the whole system; i.e., inefficient routing such as shortest path will always lead to both flows being routed along the same path, whereas other metrics may load-balance the flows across the two paths.

a table about different objectives/flow patterns/topology combinations or something.

\subsection{Mininet}
Mininet is a network emulator which uses container-based emulation \cite{handigol:mininet} to create a virtual network of hosts and switches. Mininet makes it simple to run reproducible network experiments under realistic conditions, with topologies and behaviour programmable in Python. The resources allocated to each host can be controlled and monitored, and benchmarking \cite{handigol:benchmarks} has generally shown that the performance and timing characteristics are accurate. OpenFlow-enabled switches such as OpenVSwitch can be used in the simulations so the system is ideal for SDN research.



\section{Experiment Scripts}
This section talks about the APIs and conventions etc. basic idea is you can specify topology elsewhere and import that module, same with the routing metric you want to use, then for a given experiment trial just pull in the specific stuff you want. so this section is like 'once you've decided what trials you want to run, how do you write code to make that happen'.

\subsection{Scenarios}
Have a set topology, flows and so on. hedera paper went to some effort to test different flow patterns, like 'random', 'strides' etc. i guess i just went with random and that aggregation/pod thing. 

\subsection{Controllers}
In order to properly measure certain parts of the system individually and guarantee that results are due to one specific section you need to inject mock data at certain parts of the system.

The system lets you specify callbacks to be used in the following places.

\begin{itemize}
\item forwarding rules
\item objective function
\end{itemize}

This should actually be content taken roughly from the "writing experiment scripts" chapters of the documentation, and the section which talks about individual things, what's the word, injecting mock data.

\section{Emulation Hardware}

Mininet's scalability is dependent on the specfications of the host machine running the virtual network. the network experiments in this thesis were run on a quad-core i5-4670K (3.4 GHz) desktop with 4 GB RAM. how do you write things about computers
