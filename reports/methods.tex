\chapter{Experimental Methods}
\label{ch:methods}

The goal of this thesis is to implement a configurable controller which can be used in network experiments to compare the performance of different routing algorithms. For the purposes of this thesis, performance is measured exclusively in terms of overall throughput in the network. In order to measure throughput for different routing metrics, a series of network experiments are devised. 

Chapter \ref{ch:software} outlines the layout of the controller used in the experiments. Chapter \ref{ch:algorithm} outlines the three routing metrics of interest: the shortest-path metric, given its simplicity and the number of traditional routing metrics which are based on it, can be considered a benchmark by which the other two metrics can be compared. This chapter describes the structure and implementation of the experiments.

\section{Dependencies}
\subsection{Mininet}
Mininet is a network emulator which uses container-based emulation \cite{handigol:mininet} to create a virtual network of hosts and switches. Mininet makes it simple to run reproducible network experiments under realistic conditions, with topologies and behaviour programmable in Python. The resources allocated to each host can be controlled and monitored, and benchmarking \cite{handigol:benchmarks} has shown that the performance and timing characteristics are accurate. OpenFlow-enabled switches such as OpenVSwitch can be used in the simulations so the system is ideal for SDN research.

\subsection{iperf3}
\texttt{iperf3} \cite{iperf} is a command line tool which can be used to perform network throughput measurements. Testing is performed by running a server on the destination host and a client on the source host, which will attempt to contact the server over TCP or UDP. Various options are available, including configuration of the TCP/UDP ports used, and the maximum bandwidth (size) of the flow. It is also possible to begin sending packets for a flow, but omit the first $n$ seconds from throughput calculations. Results, including the final measured throughput for each client/server pair, can be reported in JSON format for ease of futher processing.


\section{Experiment Design}
All network experiments follow a defined pattern. Figure \ref{fig:timing} shows the timing of each experiment trial. Firstly, POX and Mininet are started. There is a non-trivial setup time for Mininet topologies to be created, followed by a non-trivial time for POX's discovery module to discover the network. After waiting for this to occur (20 seconds is enough time for the largest topology considered for this thesis), a series of \texttt{iperf3} flows are started, limited to a specified bandwidth. 

\begin{figure}
  \centering
  \includegraphics[scale=0.8]{../images/timing.pdf}
  \caption{Timing of events in each trial}
  \label{fig:timing}
\end{figure}

For the first 10 seconds, \texttt{iperf3} simply sends data at a maximum of this specified bandwidth, but ignores the number of bytes sent for this period. This is because the throughput for the flow will be lower than expected, for several reasons: the TCP slow-start period, the time taken for ARP requests and replies to be exchanged, and for the controller to be notified of the hosts involved and set an initial path. 

After 10 seconds, the controller's multicommodity module recalculates all routes in the network according to the objective function used, and replaces the initial flow rules with updated forwarding rules based on these routes. The \texttt{iperf3} clients continue to send data at the same rates for this period, but begin to record the number of bytes sent and received, and do so for 10 seconds. A few seconds after this, to allow time to write this data to log files, the experiment concludes and the network is shut down, including all \texttt{iperf3} servers and clients still running.

The topology, flow pattern and routing metric considered can all be configured via a simple launch script, such as the sample shown in Figure \ref{fig:script}. The following sections explain how to firstly select these parameters so that useful information can be obtained, and secondly to write experiment scripts to run the desired experiment. 

\section{Parameter Selection}
While it is possible to use mcfpox to run experiments using many combinations of topologies and flow patterns, not all of these combinations will yield useful data. For example, if all flows in a pattern are much larger or smaller than the link capacities in the topology, either all flows will be constrained or none will be. A carefully-selected combination, however, will yield a situation where flows will only be constrained if an inefficient routing metric is used, but not otherwise. In such a situation it is possible to measure the variation in performance for the two routing metrics.

As noted in section \ref{sec:nd}, due to constraints on the ability of the controller to dynamically discover link capacities, all implemented topologies have 10 Mbps links between switches. The experiments in this thesis therefore examine flows with individual sizes ranging from 1 - 9 Mbps. The exact flow patterns used depend on the topology and are specified in Table \ref{tab:patterns}. 

Diagrams of the two topologies used in the experiments can be seen in Figures \ref{fig:pentagon} and \ref{fig:alfares}; the second topology, which was used in most of the experiments, is based on the description of a fat-tree network given in \cite{alfares:fattree}. This topology scales to much larger sizes based on a parameter $k$, the number of ports per switch; larger versions are used to compare the scalability of different routing metrics.

\begin{table}
  \centering
  \begin{tabular}{p{3cm}p{4cm}p{5cm}}
    \toprule
    Topology & Pattern & Description \\
    \midrule
    pentagon & \parbox[t]{4cm}{bidirectional($b$) \\ 2 flows \strut} & \parbox[t]{5cm}{h1 - h2, $b$ Mbps \\ h2 - h1, $b$ Mbps \\ $b = 1,2,..,10$\strut} \\
    \midrule
    \multirow{2}{*}{\vspace{0.65cm}alfares} & \parbox[t]{4cm}{pairs($b$) \\ 8 flows\strut} & \parbox[t]{5cm}{8 random pairs from h1-h16 such that each host is part of exactly one $b$ Mbps flow \\ $b = 1,2,..,10$\strut} \\
    & \parbox[t]{4cm}{random($n$,$b$) \\ $n$ flows\strut} & \parbox[t]{5cm}{$n$ random pairs from h1-h16 Hosts may appear in more than one pair or none but no pair is repeated \\ $b$ = 1,5,7} \\
    \bottomrule
  \end{tabular}
  \caption{Traffic patterns studied for each topology}
  \label{tab:patterns}
\end{table}


\section{Experiment Scripts}
One of the goals of this thesis was to create an easy-to-use experiment framework. \thesis{}, therefore, includes an experiment module which abstracts most of the boilerplate involved in setting up a routing experiment, allowing the experimenter to run the entire experiment from a script such as the one shown in Figure \ref{fig:script}.

There is a conceptual distinction, which is referenced in the structure of the script, between `scenario' and `controller' configuration. A scenario comprises a topology and a set of flows to be started at specified times. Controller configuration is where options to be passed to the controller are stored. To compare the performance of two objective functions, the scenario is kept constant while only the controller objective is changed. When flows are allocated randomly, the same allocation of flows is used for each metric, and at least 10 different random allocations are considered and then averaged, to minimise the effects of particularly difficult flow patterns.

\begin{figure}
\begin{lstlisting}
from mcfpox.topos import alfares
from mcfpox.objectives import shortest_path
from mcfpox.experiments.boilerplate import start

pairs = [('h1','h8'), ('h8','h1')]
flows = {
    19: [(i,j,5) for i,j in pairs]
}

scenario = {
    'net': alfares,
    'flows': flows
}

controller = {
    'objective': shortest_path.objective,
}

results = start(scenario, controller)
\end{lstlisting}
\caption{Sample experiment script}
\label{fig:script}
\end{figure}

\section{Emulation Hardware}

Mininet's scalability is dependent on the specfications of the host machine running the virtual network. The network experiments in this thesis were run on a quad-core i5-4670K (3.4 GHz) desktop with 4 GB RAM. 

\begin{figure}
  \centering
  \includegraphics[scale=0.8]{../images/pentagon.pdf}
  \caption{Pentagon topology (`pentagon')}
  \label{fig:pentagon}
  \vspace{2cm}
  \includegraphics[scale=0.6]{../images/alfares.pdf}
  \caption{Al-Fares fat-tree topology (`alfares')}
  \label{fig:alfares}
  This topology belongs to a class of related networks described in \cite{alfares:fattree}, parametrised by $k$, the number of ports per switch. Here, $k=4$. Larger versions of this topology are used for scalability testing of routing metrics.
\end{figure}

